{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c4cc1a1-1149-4a20-b823-32d72c2b1616",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# # Importing the project from a subfolder\u001b[39;00m\n\u001b[1;32m      8\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Aggregator\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# # Importing the project from a subfolder\n",
    "sys.path.append('./project')\n",
    "\n",
    "from models.transformer import Aggregator\n",
    "from models.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702f33de-ee68-4678-8f75-d0be5bee8c5d",
   "metadata": {},
   "source": [
    "# Test the Aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fde3b746-51e8-46fd-9515-d5164de75243",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = Aggregator(embed_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c248cd5-388b-47d1-8cd9-51a05f9c0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, S, P, C = 5, 3, 20, 64 # B events, S images per event, P tokens per image, C elements per token\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "test_input = torch.tensor(np.random.randn(B, S, P, C)).float().to(device)\n",
    "test_pos = torch.tensor(np.random.randint(1, 8, size=(B, S, P, 2))).to(device)\n",
    "test_model = test_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8af2a30-28d9-4f48-9f72-c0c4fafcd641",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output, test_idx = test_model.forward(test_input, test_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb407515-8ea4-479f-881b-3ed935f2e2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 25, 128])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba676435-e796-44a9-97d2-1f9916892c06",
   "metadata": {},
   "source": [
    "# Test the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa49665c-b066-44be-b7cb-36d6233c51b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be6265db-c060-4aac-a82a-d4d7cadfc642",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, H = 5, 16 # B total patches, each image a HxH square\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "test_input = torch.tensor(np.random.randn(B, H, H)).float().to(device)\n",
    "test_input = test_input.view(B, 1, H, H)\n",
    "test_model = test_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "073427e9-2491-4c85-a1dc-bcb72a18adc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = test_model.forward(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3af3956d-0d8d-4c41-b1e9-438e269955b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 256, 1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f72f8e4c-ace3-4b25-abeb-4679e9ecc266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 256])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output.view(5, -1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321e7738-3462-41f9-ab89-d6aa435475dd",
   "metadata": {},
   "source": [
    "# Test both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c5c046e-fd28-4228-8306-77da9838454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenizer = Tokenizer()\n",
    "test_aggregator = Aggregator(embed_dim=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30edb8c8-09f8-4fc5-9379-5444044b5f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, S, P, H = 1, 3, 5, 16 # B events, S images per event, P patches per image, HxH patches\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "test_input = torch.tensor(np.random.randn(B, S, P, H, H)).float().to(device)\n",
    "test_pos = torch.tensor(np.random.randint(1, H+1, size=(B, S, P, 2))).to(device)\n",
    "test_tokenizer, test_aggregator = test_tokenizer.to(device), test_aggregator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9995c411-e1bf-47c6-8c29-d1e0eb5ddf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple flattening for this test. In the real case this would involve recording the sequence lengths\n",
    "test_tokens = test_tokenizer.forward(test_input.view(B*S*P, H, H)).view(B, S, P, 256)\n",
    "test_output, test_idx = test_aggregator.forward(test_tokens, test_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6df4338d-3a61-4d3c-8ae3-f8bca25c723b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 10, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b020c7-d060-4304-996b-07aafb6f74bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
